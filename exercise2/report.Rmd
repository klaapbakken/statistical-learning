---
title: 'Compulsory exercise 2: Group XYZ'
author: "NN1, NN2 and NN3"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document: default
  pdf_document: default
subtitle: TMA4268 Statistical Learning V2019
  pdf_document:
    latex_engine: xelatex
    keep_tex: true
header-includes:
  - \newcommand{\matr}[1]{\boldsymbol{#1}}
  - \usepackage{bm}
  - \usepackage[a]{esvect}
  - \renewcommand{\vect}[1]{\vv{\bm{#1}}}
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error=FALSE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")
```

```{r,eval=FALSE,echo=TRUE}
install.packages("knitr") #probably already installed
install.packages("rmarkdown") #probably already installed
install.packages("bestglm")# for subset selection with categorical variables
install.packages("glmnet")# for lasso
install.packages("tree") #tree
install.packages("randomForest") #for random forest
install.packages("ElemStatLearn") #dataset in Problem 2
BiocManager::install(c("pheatmap")) #heatmap in Problem 2
```

```{r,eval=FALSE}
install.packages("ggplot2")
install.packages("GGally") # for ggpairs
install.packages("caret") #for confusion matrices
install.packages("pROC") #for ROC curves
install.packages("e1071") # for support vector machines
install.packages("nnet") # for feed forward neural networks
```

# Problem 1: Regression [6 points]

```{r}
all=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/diamond.dd")
dtrain=all$dtrain
dtest=all$dtest
```

**Q1**: Would you choose `price` or `logprice` as response variable? Justify your choice. Next, plot your choice of response pairwise with `carat`, `logcarat`, `color`, `clarity` and `cut`. Comment.

Use the local regression model $\texttt{logprice} = \beta_0 + \beta_1 \texttt{carat}+ \beta_2 \texttt{carat}^2$ weighted by the tricube kernel $K_{i0}$. 

**Q2:** What is the predicted price of a diamond weighting 1 carat. Use the closest 20% of the observations.  
**Q3:** What choice of $\beta_1$, $\beta_2$ and $K_{i0}$ would result in KNN-regression?  
  
  **Q4:** Describe how you can perform model selection in regression with AIC as criterion.  
**Q5:** What are the main differences between using AIC for model selection and using cross-validation (with mean squared test error MSE)?  
  **Q6:** See the code below for performing model selection with `bestglm()` based on AIC. What kind of contrast is used to represent `cut`, `color` and `clarity`? Write down the final best model and explain what you can interpret from the model.  
**Q7:** Calculate and report the MSE of the test set using the best model (on the scale of `logprice`).  

```{r,eval=FALSE}
library(bestglm)
ds=as.data.frame(within(dtrain,{
  y=logprice    # setting reponse
  logprice=NULL # not include as covariate
  price=NULL    # not include as covariate
  carat=NULL    # not include as covariate
}))
fit=bestglm(Xy=ds, IC="AIC")$BestModel
summary(fit)
```

**Q8:** Build a model matrix for the covariates `~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1`. What is the dimension of this matrix?  
  **Q9:** Fit a lasso regression to the diamond data with `logprice` as the response and the model matrix given in Q8. How did you find the value to be used for the regularization parameter?  
  **Q10:** Calculate and report the MSE of the test set (on the scale of `logprice`).  

**Q11:** A regression tree to model is built using a _greedy_ approach. What does that mean? Explain the strategy used for constructing a regression tree.  
**Q12:** Is a regression tree a suitable method to handle both numerical and categorical covariates? Elaborate.  
**Q13:** Fit a (full) regression tree to the diamond data with `logprice` as the response (and the same covariates as for c and d), and plot the result. Comment briefly on you findings.  
**Q14:** Calculate and report the MSE of the test set (on the scale of `logprice`).  

**Q15:** Explain the motivation behind bagging, and how bagging differs from random forest? What is the role of bootstrapping?  
  **Q16:** What are the parameter(s) to be set in random forest, and what are the rules for setting these?  
  **Q17:** Boosting is a popular method. What is the main difference between random forest and boosting?  
  **Q18:** Fit a random forest to the diamond data with `logprice` as the response (and the same covariates as before). Comment on your choice of parameter (as decribed in Q16).   
**Q19:** Make a variable importance plot and comment on the plot. Calculate and report the MSE of the test set (on the scale of `logprice`).  

**Q20:** Finally, compare the results from c (subset selection), d (lasso), e (tree) and f (random forest): Which method has given the best performance on the test set and which method has given you the best insight into the relationship between the price and the covariates?
  
  
  # Problem 2: Unsupervised learning [3 points]
  
  **Q21:** What is the definition of a principal component score, and how is the score related to the eigenvectors of the matrix ${\hat {\bf R}}$. 
  
  The data is given in the matrix $\matr{X}$, of size $n \times p$, where the columns are points in space. The columns are then scaled and shifted to mean $0$ and variance $1$, and we then have a standardized version of the observations $\matr{Z}$, of size $n \times p$. From this standardized matrix we can estimate the gene-standardized covariance matrix $\matr{\hat{R}}$ of size $p \times p$ by the equation 
  $$
  \matr{\hat{R}} = \frac{1}{n-1}\sum\limits_{i=1}^n(\matr{Z}_i- \matr{\bar{Z}})(\matr{Z}_i- \matr{\bar{Z}})^T.
  $$
  We now have a matrix of correlated variables and we want to turn it uncorrelated while keeping as much information about the data as possible. Doing this we get what is called \textit{principal component} of $\matr{\hat{R}}$ and we call it $\matr{Y}$, a $p \times M$ . This is calculated in the following way
  
$$
\begin{array}{rcl}
\matr{Y}_m & = &  \matr{\phi}_m \matr{\hat{R}}\\
||\matr{\phi}_m||_2 & = & 1 
\end{array}
$$
We can find the eigenvalues of $\matr{\hat{R}}$, and the eigenvalue decomposition is given by
$$
\matr{\hat{R}} = \matr{V} \matr{\Lambda} \matr{V}^{-1}
$$
where $\matr{V}$ holds its eigenvectors and $\matr{\Lambda}$ holds its eigenvalues on its diagonal. Then one can show that the $\matr{\phi}=\matr{V}$, which means that $\matr{\phi}$ is equal to the eigevectors of $\matr{\hat{R}}$. Which means that the \textit{principal compenent scores} is given by the new values that is transformed into the new space. The transformation is given by $\matr{Z}\matr{\phi} = \matr{Z} \matr{\phi}$.

**Q22:** Explain what is given in the plot with title "First eigenvector". Why are there only $n=64$ eigenvectors and $n=64$ principal component scores?

In Figure \ref{fig:firsteigvec} we can see the first eigenvector in the matrix $\matr{V}$ or what we found was equal to $\matr{\phi}4$. It is the eigenvector of the matrix $\matr{\hat{R}}$. It shows how to linearly combine the $\matr{Z}$ into our new transformed space. It is the first \textit{principal component}, which is the component with the largest variance. To find the number of eigenvectors we need to know the rank of the matrix $\matr{\hat{R}}$, which is $\mathrm{rank}(\matr{\hat{R}}) = min(p,n-1) = (6830,64-1) = 63$. But the \texttt{prcomp} returns 64 eigenvalue elements, where the last element is for $\lambda = 0$, so $\matr{V}_{64} = \vect{0}$.

```{r, fig.cap = "\\label{fig:firsteigvec} First eigenvector."}
library(ElemStatLearn)
X=t(nci) #n times p matrix
table(rownames(X))
ngroups=length(table(rownames(X)))
cols=rainbow(ngroups)
cols[c(4,5,7,8,14)] = "black"
pch.vec = rep(4,14)
pch.vec[c(4,5,7,8,14)] = 15:19

colsvsnames=cbind(cols,sort(unique(rownames(X))))
colsamples=cols[match(rownames(X),colsvsnames[,2])] 
pchvsnames=cbind(pch.vec,sort(unique(rownames(X))))
pchsamples=pch.vec[match(rownames(X),pchvsnames[,2])]

Z=scale(X)

pca=prcomp(Z)

plot(1:dim(X)[2],pca$rotation[,1],type="l",xlab="genes",ylab="weight",main="First eigenvector")
```

  **Q23:** How many principal components are needed to explain 80% of the total variance in ${\bf Z}$? Why is `sum(pca$sdev^2)=p`?  
  
  By the transformation the fraction of the variance is found by the equation
  
$$
R^2 =\frac{\sum_{i=1}^M \lambda_i}{\sum_{j=1}^p \lambda_j}  
$$
To find at which number of \textit{principal components}, they explain $80\%$ of to total variance we need to write some code to calculate it. And this is done in the code below. The result of which is $\textrm{# PC}$``r res_i``. 
The reason `sum(pca$sdev^2)=p` is because of our scaling. The variance has been scaled to $1$ and therefore by the $\sum_{i=1}^p 1 = p$.



```{r}
# calculating the number of PC to explain 80% of the variance
quotient_variance <- cumsum(pca$sdev^2)/sum(pca$sdev^2)
for (i in seq(1,length(quotient_variance))){
  if (quotient_variance[i]>=0.8){
    res_i = i
    break
  }
}
```

  **Q24**: Study the PC1 vs PC2 plot, and comment on the groupings observed. What can you say about the placement of the `K262`, `MCF7` and `UNKNOWN` samples? Produce the same plot for two other pairs of PCs and comment on your observations.

In Figure \ref{fig:pc1pc2} we see the first \textit{principal component} on the x-axis versus the second \textit{principal component} on the y-axis. The `K562` cells, that we know are part of the `LEUKEMIA` familiy, is clustered with exactly these cells for both versions \textit{A} and \textit{B}. `MCF7` cells is part of the `BREAST` cancer famility. In the figure it is clusterted together with one of these cells, but also together with `COLON`Cells. The `BREAST`cancer cells doesn't really have any clustered effects in this plot, but the `COLON`cells are very clustered in the top right corner. In the top left corner there is a big cluster of many different cancer cells, `NSCLC`, `OVARIAN` and `RENAL` in particular. And within this cluster the `UNKnOWN`cells is placed. Which means that we can't really give a clear indication of which cancer familiy the `UNKNOWN` cells are part of. 

We plotted the `PC32` vs. `PC33` in Figure \ref{fig:pc32pc33} for when the \textit{principal components} explain $80\%$ of the variance. We also plotted the `PC63` vs. `PC64` for when the variance is as small as possible and when the null vector to transform the principal component is used. 
In these plots we can't see any significant clustering, that would classifiy the cancer cells in any perticular way. 

```{r, fig.cap = "\\label{fig:pc1pc2} PC1 vs. PC2"}
plot(pca$x[,1],pca$x[,2],xlab="PC1",ylab="PC2",pch=pchsamples,col=colsamples)
legend("bottomright",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)
```

```{r, fig.cap = "\\label{fig:pc32pc33} PC32 vs. PC33"}
plot(pca$x[,32],pca$x[,33],xlab="PC32",ylab="PC33",pch=pchsamples,col=colsamples)
legend("bottomright",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)
```

```{r, fig.cap = "\\label{fig:pc63pc64} PC63 vs. PC65"}
plot(pca$x[,63],pca$x[,64],xlab="PC63",ylab="PC64",pch=pchsamples,col=colsamples)
legend("bottomright",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)
```


**Q25:**: Explain what it means to use Euclidean distance and average linkage for hierarchical clustering. 

We first look at the average linkage. The algorithm for hierarchical clustering is given by

\begin{enumerate}
\item\textbf{Given:}
\item A set of $n$ observations \matr{X}
\item A dissimiliarity function \texttt{f(x)}
\item \texttt{While #clusters > 1; Do:}
\begin{enumerate}
\item Find two clusters that are the least dissimilar
\item Create new cluster of these
\item Remove remove previous cluster
\item \texttt{End do;}
\end{enumerate}
\end{enumerate}

In this algoritm we see that we need some kind of dissimiliarity measure, and in this exercise we use the average linkage. The average linkage is in plain words the \textit{mean inter-cluster dissimiliarity}, given by the equation
$$
L(l,r) = \frac{1}{n_l n_r}\sum\limits_{i = 1}^{n_l}\sum\limits_{j=1}^{n_r}||\vect{x}_{li}-\vect{x}_{rj}||_2 .
$$
Where $l$ is the set of points in one cluster and $r$ is the set of points in the second cluster. The dissimilarity calculation inside the $||\cdot||_2$ is in our case the Euclidean distance. This is to capture the physical distance between two points in the space. Within a cluster one wants to minimize the variance between the observations to find the best possible cluster for each observation. This is done by the following equation, also using the Euclidean distance as a dissimilarity measure, 
$$
 L(C_k) = \frac{1}{n_{C_k}} \sum\limits_{i,j\in C_k} \sum\limits_{p=1}^P ||x_{ip}-x_{jp}||_2^2.
$$
with $n_{C_k}$ being the number of points in cluster number $k$, with the points having dimension $P$. The Euclidean distance is given by $||x_1- x_2||_2 = \sqrt{(x_1^2 + x_2^2)}$. Using this on the scaled observations $\matr{Z}$, one would end up with the partition shown in Figure \ref{fig:dendogram}.

**Q26:**: Perform hierarchical clustering with Euclidean distance and average linkage on the scaled gene expression in `Z`. Observe where our samples labelled as `K562`, `MCF7` and `UNKNOWN` are placed in the dendrogram. Which conclusions can you draw from this?
```{r, fig.cap = "\\label{fig:dendogram} Dendogram of hierarchical clustering with euclidean distance and average linkage on \matr{Z}"}
library(ggplot2)
library(ggdendro)
library(tidyverse)
library(dendextend)
hc.average=hclust(dist(Z), method="ave")
str(hc.average)
dhc <- as.dendrogram(hc.average,hang=FALSE,dLeaf = NULL)
ddata <- dendro_data(dhc, type = "rectangle")
tmpx = round(segment(ddata)$xend)
tmpy = segment(ddata)$yend
res = numeric(length = length(label(ddata)$x))+300
for (i in seq(1,length(tmpx))){
  if (res[tmpx[i]]>tmpy[i]){
     res[tmpx[i]] = tmpy[i]
  }
}
res
col.text <- rep("red",64)
col.text[(label(ddata)$label == "UNKNOWN")|
           (label(ddata)$label == "K562A-repro")|
           (label(ddata)$label == "K562B-repro")|
           (label(ddata)$label == "MCF7A-repro")|
           (label(ddata)$label == "MCF7D-repro")] = "blue"

p <- ggplot(segment(ddata))+
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend))+
  geom_text(data = label(ddata), angle = 90,
            aes(x = x, y = res, label = label),colour = col.text, vjust = 0.5,hjust = 1, size = 2)+
  scale_colour_discrete(c("blue","red"))+
  ylim(c(0,max(segment(ddata)$y)))+
  theme_minimal()+
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks = element_blank(),
        axis.line.y = element_line(color = "black", 
                                   size = 1, linetype = "solid"))
  
p
```
The `K562` cells are clearly clustered together with only the `LEUKEMIA` cells, which is what we should expect because `K562` is in the `LEUKEMIA`family. Looking at the `MCF7` they are clustered together with `BREAST` cells, though these cells are spread out on the right side of the tree, it fits the fact that we know they `MCF7` are part of the `BREAST` familiy. The `UKNOWN`cells are clustered together with the `OVARIAN` cells, it also branches in to a bigger cluster of `OVARIAN` leafs. It is also in close connection to `NSCLC` and `MELANOMA`, but looks to either be a `OVARIAN` cell or in familiy with. 
  
  **Q27:**: Study the R-code and plot below. Here we have performed hierarchical clustering based on the first 64 principal component instead of the gene expression data in `Z`. What is the difference between using all the gene expression data and using the first 64 principal components in the clustering? We have plotted the dendrogram together with a heatmap of the data. Explain what is shown in the heatmap. What is given on the horizontal axis, vertical axis, value in the pixel grid?

From earlier questions we know that the principal components can be found by the equation $\vect{y}_k = \vect{z}_k \matr{V}$. The most significant difference between using the principal components versus the scaled observations, is computation time. The distanes betweeen points in the PC-space is the same as in the orthogonal space, because of the orthonormal matrix. 

In the figure below the result of running the hierarchical clustering algorithm with the principal components is shown. On the left side of the figure a dendogram explaining the partitioning is displayed. The rows in both the dendogram and the heatmap corresponnds to the names of the cells on the vertical axes to the right in the figure. Along the horistonal axes the principal components are displayed, with `PC1` furthest to the left. The principal components and the cell types form a grid system in the main plot, and the color of which corresponds to the \textit{principal component scores}. Red color indicates high positive scores and blue colors indicate high negative scores while zero score is a light yellow or white color. We can by this knowledge say that there are more variances in the first few principal components and they become more similiar in the later principal components, with the last elements $64$ having $0$ variance, which corresponds to the points having the same color. This what we concluded earlier in the exercise, only explained visually. 
```{r}
library(pheatmap)
npcs=64 
pheatmap(pca$x[,1:npcs],scale="none",cluster_col=FALSE,cluster_row=TRUE,clustering_distance_rows = "euclidean",clustering_method="average",fontsize_row=5,fontsize_col=5)
```

# Problem 3: Flying solo with diabetes data [6 points]

```{r}
flying=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/flying.dd")
ctrain=flying$ctrain
ctest=flying$ctest
```

**Q28:** Start by getting to know the _training data_, by producing summaries and plots. Write a few sentences about what you observe and include your top 3 informative plots and/or outputs.

# Exploratory data analysis

We start off by looking at the correlation matrix, seeing as this can provide insight into the correlation between the different covariates.  High correlation might indicate that two covariates encode similar information and that it might be beneficial to drop one of the covariates in order to reduce the complexity and variance. From the figure below it's clear that $\textrm{skin}$ and $\textrm{bmi}$ is very highly correlated, as well as $\textrm{age}$ and $\textrm{npreg}$.

```{r, fig.cap="Correlation matrix"}
library(dplyr)
library(tibble)
library(magrittr)
library(forcats)
library(purrr)
library(ggplot2)
library(tidyr)

train_df <- ctrain %>%
  as_tibble() %>%
  mutate(diabetes = as_factor(diabetes))

train_df %>%
  GGally::ggcorr()
```
We proceed by splitting the dataset into two parts, one where $\textrm{diabetes} = 1$ and another one where $\textrm{diabetes} = 0$. By creating boxplots for each of these cases we can inspect if there's any observable difference between the mean and variance of the covariates when the data is grouped by the value of $\textrm{diabetes}$. We immediately observe that the mean $\textrm{glu}$ is considerably higher for the group where $\textrm{diabetes} = 1$. The differences are not as clear for the other covariates, but it's worth noting that the mean value of $\textrm{skin}$ and $\textrm{bp}$ also seems to higher for the group with $\textrm{diabetes} = 1$.

```{r, fig.cap="Boxplot for dataset grouped by $\textrm{diabetes}$"}

train_df %>%
  gather("variable", "value", c(-diabetes)) %>%
  ggplot(aes(y=value, x=variable)) +
  geom_boxplot() + 
  facet_wrap(~diabetes)
```
The next plot provides further insight into the importance of $\textrm{glu}$ and $\textrm{skin}$. There's a clear structure in the plot below, showing that the fraction of people with diabetes increases as the values of $\textrm{skin}$ and $\textrm{glu}$ increase.
There's 

```{r}
train_df %>%
  ggplot(aes(x = glu, y = skin, color = diabetes)) +
  geom_jitter()
```




**Q29:** Use different methods to analyse the data. In particular use 

* one method from Module 4: Classification
* one method from Module 8: Trees (and forests)
* one method from Module 9: Support vector machines and, finally
* one method from Module 11: Neural networks

For each method you 

* clearly write out the model and model assumptions for the method
* explain how any tuning parameters are chosen or model selection is performed
* report (any) insight into the interpretation of the fitted model
* evaluate the model using the test data, and report misclassifiation rate (cut-off 0.5 on probability) and plot ROC-curves and give the AUC (for method where class probabilities are given).

We start off by defining a number of functions that simplify tuning through the use of cross - validation. 

```{r}
#Input: A parsnip model, from the package parsnip, and a data frame where
#each row corresponds to a CV - iteration, i.e. a specific validation set.
#The folds dataframe is a modified version of the dataframe returned by
#vfold_cv from the rsample - package.

#Output: The folds dataframe, but this with columns containing 1) models fitted on the training data
#for the corresponding iteration and 2) predictions on the corresponding validation set

predict_on_folds <- function(model, folds) {
  prediction_df <- folds %>%
    mutate(
      fitted_model =
        map2(
          recipe,
          train,
          ~ model %>%
            fit(
              formula(.x),
              data = bake(
                object = .x,
                new_data = .y
              ),
            )
        ),
      prediction =
        pmap(
          lst(
            recipe,
            test,
            fitted_model
          ),
          function(recipe, test, fitted_model)
            fitted_model %>%
              predict(
                new_data = bake(
                  object = recipe,
                  new_data = test
                ),
                type = "class"
              )
        ),
      test_estimate = prediction %>%
        map(".pred_class")
    )
  return(prediction_df)
}


# Input: Data frame containing truths and estimates in each row
# Output: Average accuracy across all rows
validation_score <- function(cv_df) {
  cv_df %>%
    select(test_truth, test_estimate) %>%
    pmap(
      function(
                     test_truth,
                     test_estimate)
        tibble(
          test_truth,
          test_estimate
        )
    ) %>%
    map(accuracy,
      truth = test_truth,
      estimate = test_estimate
    ) %>%
    map_dbl(".estimate") %>%
    mean()
}
```

We do some additional preparation that simplify preprocessing and ensures that the input is consistent across all methods. We define two different "recipes", one that centers and scales the data have mean zero and unit variance and one that does not. 

```{r}
library(recipes)
library(rsample)

test_df <- ctest %>%
  as_tibble() %>%
  mutate(diabetes = as_factor(diabetes))

unscaled_rec <- train_df %>%
  recipe() %>%
  update_role(diabetes, new_role = "outcome") %>%
  update_role(-diabetes, new_role = "predictor")

unscaled_prepped <- unscaled_rec %>%
  prep(retain = TRUE)

train_unscaled <- unscaled_prepped %>%
  juice()

test_unscaled <- unscaled_prepped %>%
  bake(test_df)

unscaled_folds <- train_df %>%
  vfold_cv(v = 5) %>%
  mutate(
    recipe = splits %>%
      map(prepper, recipe = unscaled_rec),
    train = splits %>%
      map(analysis),
    test = splits %>%
      map(assessment),
    test_truth = test %>%
      map("diabetes")
  )

scaled_rec <- train_df %>%
  recipe() %>%
  update_role(diabetes, new_role = "outcome") %>%
  update_role(-diabetes, new_role = "predictor") %>%
  step_center(-diabetes) %>%
  step_scale(-diabetes)


scaled_prepped <- scaled_rec %>%
  prep(retain = TRUE)

train_scaled <- scaled_prepped %>%
  juice()

test_df <- ctest %>%
  as_tibble() %>%
  mutate(diabetes = as_factor(diabetes))

test_scaled <- scaled_prepped %>%
  bake(test_df)

scaled_folds <- train_df %>%
  vfold_cv(v = 5) %>%
  mutate(
    recipe = splits %>%
      map(prepper, recipe = scaled_rec),
    train = splits %>%
      map(analysis),
    test = splits %>%
      map(assessment),
    test_truth = test %>%
      map("diabetes")
  )
```



#Logistic regression

The first method we want to explore is logistic regression. Logstic regression assumes that the response takes on values in $\{0, 1\}$. In addition, we assume that $Y_i, i = 1,\ldots, n$ is Bernoulli - distributed with sucess probability $p_i$. The logistic function has the form

$$
p_i = \frac{\exp{\left(\beta_0 + \sum_{n=1}^p\beta_px_p\right)}}{1 + \exp{\left(\beta_0 + \sum_{n=1}^p\beta_px_p\right)}},
$$
where $p$ is the number of covariates. In our case we have $p = 7$.  The different values of $\mathbf{\beta}$ are obtained by maximizing the likelihood function $L(\mathbf{\beta} \mid \ \mathbf{x}) = \prod_{i=1}^nf(x_i \mid \mathbf{\beta})$, where $f(x_i \mid \mathbf{\beta})$ is the Bernoulli - pmf with parameter $p_i$ as shown above. The maximation is done using a numerical optimization algorithm - typically by the use of either Fisher scoring or the Newton-Rahpson method. 

It's not necessarily desirable to include all covariates in the model. In order to find a model that performs well without being unnecessarily complex we calculate the AIC for the possible models and choose the model that maximizes AIC as the final model. 


```{r}
library(bestglm)
library(yardstick)

best_glm <- bestglm::bestglm(
  ctrain %>%
    select(-diabetes, diabetes),
  family = binomial,
  IC = "AIC"
)$BestModel

summary(best_glm)

broom::tidy(best_glm) %>% select(term, estimate, p.value) %>% knitr::kable()
```

From the estimated values $\mathbf{\beta}$ shown above we can note that increased values for any of the covariates lead to higher values of $p_i$. It can also be seen that not all of the included covariates are significant. It's clear from the estimates and the related standard errors that $\textrm{bmi}$ and $\textrm{glu}$, which was identified as important during the exploratory phase, are important. It's worth noting that $\textrm{skin}$ is excluded, but this might make sense seeing as it was highly correlated with $\textrm{bmi}$. 

In the following snippet of code we test our model on the test set and report the accuracy and the AUC. We also include the confidence matrix and the ROC curve.

```{r}
class_pred <- tibble(
  estimate = best_glm %>%
    predict(
      ctest %>%
        select(
          -diabetes, diabetes
        ),
      type = "response"
    ) %>%
    map_dbl(~ .x > 0.5) %>%
    as.factor(),
  truth = test_df %>%
    pull(diabetes)
)

class_prob_pred <- tibble(
  estimate = best_glm %>%
    predict(
      ctest %>%
        select(
          -diabetes, diabetes
        ),
      type = "response"
    ),
  truth = test_df %>%
    pull(diabetes)
)

accuracy(class_pred, estimate, truth) %>%
  select(.metric, .estimate) %>%
  knitr::kable(col.names = c("Metric", "Estimate"))

roc_curve(class_prob_pred, truth, estimate) %>%
  autoplot()

roc_auc(class_prob_pred, truth, estimate) %>%
  select(.metric, .estimate) %>%
  knitr::kable(col.names = c("Metric", "Estimate"))

conf_mat(class_pred, truth, estimate) %>%
  autoplot("heatmap")
```

#Random forest

We proceed by fitting a random forest to our data. A random forest consists $B$ decision trees. The decision tree has parameters determing how many samples there needs to be in a leaf node before we can split further. This is a tuning parameter, called $\texttt{min_n}$ in the code below. Only $m, m < p$ randomly chosen covariates can be considered at each split. This is also a tuning parameter, called $\texttt{mtry}$ below. When splitting a node we want to minimize the Gini index, given by

$$
G = \sum_{k=1}^K\hat{p}_{jk}(1-\hat{p}_{jk})
$$
for leaf node $j$. 

The parameters are found through the use of K - fold cross validation, with $K = 5$. The parameter search is by no means exhaustive, but the should be sufficient for our purpose. The number of trees is fixed at $B = 100$, seeing as this is not a tuning parameter and it's common to simple set to be "high enough". By taking a look at the mean decrease in the Gini index for the diffiferent covariates, we observe that $\textrm{bmi}$ and $\textrm{glu}$ are, again, very important. The covariates $\text{ped}$ and $\text{ped}$ also seem to be fairly important.

```{r}
library(parsnip)

n_predictors <- unscaled_rec$var_info %>%
  filter(role == "predictor") %>%
  nrow()

# Defining grid used for hyperparameter search.
rf_param_df <- cross_df(list(
  mtry = seq(1, n_predictors),
  trees = list(100),
  min_n = seq(1, 20, 1)
))

# Creating a list of models corresponding to the different choices
# of hyperparameter values
model_list <- rf_param_df %>%
  as.list() %>%
  pmap(
    function(
                 mtry,
                 trees,
                 min_n)
      rand_forest(
        mode = "classification",
        mtry = mtry,
        trees = trees,
        min_n = min_n
      ) %>%
        set_engine("randomForest")
  )

# Fitting the different models
cv_list <- model_list %>%
  map(predict_on_folds,
    folds = unscaled_folds
  )


# Evaluating the performance of the different models by extracting the
# mean accuracy across all folds
rf_param_df <- rf_param_df %>% mutate(accuracy = cv_list %>%
  map_dbl(validation_score))

best_accuracy <- rf_param_df %>%
  filter(accuracy == max(accuracy)) %>%
  select(accuracy)

best_params <- rf_param_df %>%
  mutate(index = row_number()) %>%
  filter(accuracy == max(accuracy)) %>%
  select(mtry, trees, min_n) %>%
  head(1)

best_params %>% knitr::kable()


# Fitting a model using the entire test set, with
# parameters as found through cross-validation
rf_estimator <- rand_forest(
  mode = "classification",
  mtry = best_params %>% pull(mtry),
  trees = best_params %>% pull(trees),
  min_n = best_params %>% pull(min_n)
) %>%
  set_engine("randomForest") %>%
  fit(
    formula = formula(unscaled_prepped),
    data = train_unscaled
  )

rf_estimator$fit$importance %>% knitr::kable()
```

The accuracy and AUC is shown below, together with plots of the ROC curve and the confusion matrix.


```{r}


results <- tibble(
  estimated_class = rf_estimator %>%
    predict(
      new_data = test_unscaled,
      type = "class"
    ) %>%
    pull(.pred_class),
  estimated_class_probs = rf_estimator %>%
    predict(
      new_data = test_unscaled,
      type = "prob"
    ) %>%
    pull(.pred_1),
  truth = test_unscaled %>%
    pull(diabetes)
)

accuracy(results, truth, estimated_class) %>%
  select(.metric, .estimate) %>%
  knitr::kable(col.names = c("Metric", "Estimate"))


roc_auc(results, truth, estimated_class_probs) %>%
  select(.metric, .estimate) %>%
  knitr::kable(col.names = c("Metric", "Estimate"))

roc_curve(results, truth, estimated_class_probs) %>%
  autoplot()

conf_mat(results, truth, estimated_class) %>%
  autoplot("heatmap")
```
#Support vector machine

We proceed by considering a support vector classifier. This algorithms constructs a hyperplane that attempts to separate the data. The hyperplane is constructed by maximizing the margin $M$ subject to a number a constraints. 

The hyperplane is defined by $f(\mathbf{x} \mid \mathbf{\beta}) = \beta_0 + \sum_{i=1}^n\beta_ix_i = 0$. By calculating $\textrm{sign}\left(f(\mathbf{x} \mid \mathbf{\beta})\mid_{\mathbf{x}=\mathbf{x}_{i}}\right)$ we obtain the predicted class, given that classes are encoded as $Y \in \{-1, 1\}$. In other words, the observations are classified by looking at which side of the hyperplane the observation is located. 

Based on our observation in the first section, where we observed that $\text{skin}$ and $\text{skin}$ seems to increase the chance of $\text{diabetes} = 0$, we choose a polynomial decision boundary. The scatter plot of the mentioned covariates indicates that a linear decision boundary could do a good job of predicting the value of $\text{diabetes}$. 

The procedure for finding the hyperplane consists of maximizing the margin $M$ subject to a set of constraints. The margin $M$ determines how far away from the hyperplane a point must be before it stops having an impact on the construction of the hyperplane. The related optimization problem becomes 

$$
\max_{\mathbf{\beta}, \mathbf{\epsilon}}M\quad\text{ subject to} \\
\mathbf{\beta}^T\mathbf{\beta} = 1 \\
y_i(\beta_0 + \sum_{i=1}^n\beta_ix_{ip}) \geq M(1 - \epsilon_i) \ \forall i = 1,\ldots,n\\
\epsilon_i \geq 0 \\
\sum_{i=1}^n\epsilon_i \leq C
$$

We use K - fold cross validation to determine the parameters, with $K = 5$. 

```{r}

svm_param_df <- cross_df(
  list(
    cost = map(seq(-3, 2, 1), ~ 10^(.)),
    degree = seq(1, 4)
  )
)

model_list <- svm_param_df %>%
  as.list() %>%
  pmap(
    function(
                 cost,
                 degree)
      svm_poly(
        mode = "classification",
        cost = cost,
        degree = degree
      ) %>%
        set_engine("kernlab")
  )

cv_list <- model_list %>% map(predict_on_folds,
  folds = scaled_folds
)

svm_param_df <- svm_param_df %>%
  mutate(accuracy = cv_list %>%
    map_dbl(validation_score))

svm_param_df %>% filter(accuracy == max(accuracy))

best_accuracy <- svm_param_df %>% filter(accuracy == max(accuracy)) %>% select(accuracy)

best_params <- svm_param_df %>%
  mutate(index = row_number()) %>%
  filter(accuracy == max(accuracy)) %>%
  select(cost, degree) %>%
  head(1)

svm_estimator <- svm_poly(
  mode = "classification",
  cost = best_params %>% pull(cost),
  degree = best_params %>% pull(degree)
) %>%
  set_engine("kernlab") %>%
  fit(
    formula = formula(scaled_prepped),
    data = train_scaled
  )


results <- tibble(
  estimated_class = svm_estimator %>%
    predict(
      new_data = test_scaled,
      type = "class"
    ) %>%
    pull(.pred_class),
  estimated_class_probs = svm_estimator %>%
    predict(
      new_data = test_scaled,
      type = "prob"
    ) %>%
    pull(.pred_1),
  truth = test_scaled %>%
    pull(diabetes)
)

accuracy(results, truth, estimated_class) %>%
  select(.metric, .estimate) %>%
  knitr::kable(col.names = c("Metric", "Estimate"))


roc_auc(results, truth, estimated_class_probs) %>%
  select(.metric, .estimate) %>%
  knitr::kable(col.names = c("Metric", "Estimate"))

roc_curve(results, truth, estimated_class_probs) %>%
  autoplot()

conf_mat(results, truth, estimated_class) %>%
  autoplot("heatmap")
```

#Neural network

A fully-connected feedforward neural network consists of a series of hidden layers, with each layer consisting of a given number of units. Given an input vector $\mathbf{X}$, each unit in the first layer computes an output $\phi\left(\mathbf{X}\mathbf{w}_i + b_i\right)$. The function $\phi$ is referred to as the activation function. The output of the entire layer is then either passed to another hidden layer, where the process is repeated, or used as the final output. The weights of the different layers in the network, $\mathbf{W} = \begin{bmatrix} w_1,\ldots,w_n\end{bmatrix}^T$ and $\mathbf{b}$, can be estimated by minimizing a loss function with respect to the weights. This is done by computing the gradient of the loss function through a technique known as backpropagation, and applying a numerical optimization method in order to minimize the loss function. 

When fitting the neural network to diabetes - classification problem we use the rectified linear unit $\phi(x) = \max(0, x)$ as the activation function in the hidden layers and the sigmoid function $\phi(x) = \frac{1}{1 + e^{-x}}$ in the output layer. 
We use binary crossentropy as the loss function, seeing as this is the common choice for binary classification tasks. In order to avoid overfitting, which is a common problem with neural networks, we stop our training as seen as the loss computed on the validation set starts declining. 


```{r, eval=FALSE}
library(keras)
use_condaenv("base")

nn_train <- train_scaled %>%
  mutate(id = row_number())

nn_val <- nn_train %>%
  sample_frac(size = 0.2)

nn_train <- nn_train %>%
  anti_join(nn_val, by = "id")

input_tensor <- layer_input(shape = ncol(nn_train %>% select(-c(diabetes, id))))
output_tensor <- input_tensor %>%
  layer_dense(units = 8, activation = "relu") %>%
  layer_dense(units = 4, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model <- keras_model(input_tensor, output_tensor)
summary(model)

model %>% compile("Adam", loss = "binary_crossentropy", metrics = c("accuracy"))

history <- model %>% fit(nn_train %>% select(-c(diabetes, id)) %>% as.matrix(),
  nn_train %>% select(diabetes) %>% as.matrix(),
  validation_data = list(
    nn_val %>% select(-c(diabetes, id)) %>% as.matrix(),
    nn_val %>% select(diabetes) %>% as.matrix()
  ),
  callbacks = list(callback_early_stopping()),
  batch_size = 1, epochs = 50,
  view_metrics = FALSE
)

model %>% evaluate(
  test_scaled %>% select(-diabetes) %>% as.matrix(),
  test_scaled %>% select(diabetes) %>% as.matrix()
)

(model %>% predict(
  test_scaled %>% select(-diabetes) %>% as.matrix()
) %>% map_int(~.x > 0.5) == test_scaled %>% pull(diabetes)) %>% mean() 

history %>% plot() + xlim(0, length(history$metrics$acc))
```


**Q30:** Conclude with choosing a winning method, and explain why you mean that this is the winning method.

Logistic regression achieves the highest accuracy and the highest AUC. The other methods are not far behind, but has additional disadvantages that make them less attractive. Logistic regression is fairly easy to interpret due to relation between the success probability and the values of the covariates. The random forest is harder to interpret due to the high number of trees that are involved. The random nature of the random forest is also a downside. The support vector classifier is very easy to interpret for $p = 2$ and $p = 3$, but becomes harder to both visualize and explain for high-dimensional data. Neural networks are notoriously hard to interpret due to it's very complex structure. This, in combination with it's inability to reliably perform better than the logistic regression, makes it less desirable than the logistic regression in this case. When taking all of this into account we're led to choose the logistic regression as the best model for this particular problem. 