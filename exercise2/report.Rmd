---
title: 'Compulsory exercise 2: Group XYZ'
author: "NN1, NN2 and NN3"
date: "`r format(Sys.time(), '%d %B, %Y')`"
subtitle: TMA4268 Statistical Learning V2019
output:
  html_document: default
  pdf_document:
    latex_engine: xelatex
    keep_tex: true
header-includes:
  - \newcommand{\matr}[1]{\boldsymbol{#1}}
  - \usepackage{bm}
  - \usepackage[a]{esvect}
  - \newcommand{\vect}[1]{\vv{\bm{#1}}}
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error=FALSE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")
```

```{r,eval=FALSE,echo=TRUE}
install.packages("knitr") #probably already installed
install.packages("rmarkdown") #probably already installed
install.packages("bestglm")# for subset selection with categorical variables
install.packages("glmnet")# for lasso
install.packages("tree") #tree
install.packages("randomForest") #for random forest
install.packages("ElemStatLearn") #dataset in Problem 2
BiocManager::install(c("pheatmap")) #heatmap in Problem 2
```

```{r,eval=FALSE}
install.packages("ggplot2")
install.packages("GGally") # for ggpairs
install.packages("caret") #for confusion matrices
install.packages("pROC") #for ROC curves
install.packages("e1071") # for support vector machines
install.packages("nnet") # for feed forward neural networks
```

```{r}
library(tidyverse)
library(bestglm)
library(glmnet)
library(tree)
library(randomForest)
```


# Problem 1: Regression [6 points]

```{r}
all=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/diamond.dd")
dtrain=as_tibble(all$dtrain)
dtest=as_tibble(all$dtest)
```

**Q1**: Would you choose `price` or `logprice` as response variable? Justify your choice. Next, plot your choice of response pairwise with `carat`, `logcarat`, `color`, `clarity` and `cut`. Comment.

To determine if we should use `price` or `logprice` as response variable we would
like to fit a model an then assess our assumptions. This can be e.g. iid Gaussian
residuals for the standard linear model. But we are going to fit different models
with different (or no) such assumptions. Since we are going to fit a linear
model (using lasso), and we see from the residuals below that `logprice` is 
"closer" to being a
linear function of the covariates than `price`, we choose to work with 
`logprice` as our response.

```{r}
lm_price = lm(price ~ . -logprice -logcarat, dtrain)
lm_logprice = lm(logprice ~ . -price -carat, dtrain)
# Code adapted from Module 3 example
ggplot(lm_price, aes(.fitted, .stdresid)) + 
  geom_point(pch = 21) + 
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") + 
  labs(x = "Fitted values", y = "Standardized residuals", 
       title = "Fitted values vs standardized residuals", 
       subtitle = deparse(lm_price$call))
ggplot(lm_logprice, aes(.fitted, .stdresid)) + 
  geom_point(pch = 21) + 
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") + 
  labs(x = "Fitted values", y = "Standardized residuals", 
       title = "Fitted values vs standardized residuals", 
       subtitle = deparse(lm_logprice$call))
```

We plot `logprice` pairwise with `carat`, `logcarat`, `color`, `clarity` and 
`cut`. If there is any connection between the price and `carat`, `logcarat` 
and `color`, it is not easy to see from these plots. There is clearly a 
connection between the carats and the price, however; `logprice` seems to 
increase approximately linearly with `logcarat`.

```{r}
dtrain_log = select(dtrain, -price)
dtrain_log %>%
  mutate_if(is.factor, as.character) %>%
  gather(key, value, cut, color, clarity) %>%
  ggplot(aes(value, logprice)) +
    facet_wrap(~key, scales='free') +
    geom_point(alpha=0.1, col="blue")
dtrain_log %>%
  gather(key, value, carat, logcarat) %>%
  ggplot(aes(value, logprice)) +
    facet_wrap(~key, scales='free') +
    geom_point(alpha=0.1, col="blue")
```

Use the local regression model 
$\texttt{logprice} = \beta_0 + \beta_1 \texttt{carat}+ \beta_2 \texttt{carat}^2$
weighted by the tricube kernel $K_{i0}$. 

We use the local regression model $\texttt{logprice} = \beta_0 + \beta_1 
\texttt{carat} + \beta_2 \texttt{carat}^2$ weighted by the tricube kernel

$$
K_{i0} = \left( 1 - \left|\frac{x_0 - x_i}{x_0 - x_k}\right|^3 \right)^3_+ \, ,
$$
where $x_k$ is the $k$th closest observation to $x_0$. This is implemented in 
the following code (with $k =$ 20\% of the total number of observations).

```{r}
fit_lr = loess(logprice ~ carat, data=dtrain_log, span=0.2, degree=2)
tibble(carat=dtrain$carat, logprice=dtrain$logprice, fitted=fit_lr$fitted) %>%
  ggplot() +
    geom_point(aes(carat, logprice), alpha=0.1, col="blue") +
    geom_line(aes(carat, fitted), col="darkred")
```


**Q2:** What is the predicted price of a diamond weighting 1 carat. Use the closest 20% of the observations.

The predicted logprice of a diamond weighing 1 carat is 
`r predict(fit_lr, 1)`. The standard error is 
`r predict(fit_lr, 1, se=T)$se.fit`. The predicted price is 
`r 10**predict(fit_lr, 1)`.

**Q3:** What choice of $\beta_1$, $\beta_2$ and $K_{i0}$ would result in KNN-regression?  

$\beta_1 = \beta_2 = 0$ and $K_{i0} = 1$ would result in KNN-regression,
since then $\hat{\beta}_0 = \frac{1}{k} \sum_{i \in \mathcal{N}_0} y_i$ is the
estimate of `logprice` in $x_0$ ($\mathcal{N}_0$ is the $k$-neighborhood of 
$x_0$).
  
**Q4:** Describe how you can perform model selection in regression with AIC as criterion.  

AIC is a criterion for (linear) model selection that penalizes the training error
according to how complex the model is; 
$AIC = \frac{1}{n\hat{\sigma}^2}(RSS + 2d\hat{\sigma}^2)$, where d is the number
of predictors in the model. We could fit all possible models from subsets of the 
predictors we consider. However, if we have a large number of predictors (and
possibly functions of them), this could be computationally intractable, and it
could lead to overfitting since we by chance could get a very good score for some
models. Alternatively we could use stepwise selection. Then we start with either
all predictors in the model and remove the one increasing AIC the most until we
we can't remove any more, or go the other way, or do a mix (at each step either
add or remove a predictor based on AIC).

**Q5:** What are the main differences between using AIC for model selection and using cross-validation (with mean squared test error MSE)?  

AIC estimates the test error by penalizing the training error. With
cross-validation we divide the data into $k$, say $k=5$, parts, then train (fit) on all
but one and predict on the one left out of training (the validation set). Then
we repeat this four more times, leaving each part out of training once. Thus we
get a more direct estimate of the test error. And we get several estimates for
the same model, giving an estimate of the variance of our estimated test error.
The estimated test error is biased; it generally overestimates the test error, 
since we don't train on all the data. Cross-validation for model selection is
computationally expensive (we have to fit each model several times). However, it
makes fewer (no) assumptions about the true underlying model, and we don't have
to pinpoint the number of degrees of freedom of the model (which can be hard for
other models than standard linear models).

**Q6:** See the code below for performing model selection with `bestglm()` based on AIC. What kind of contrast is used to represent `cut`, `color` and `clarity`? Write down the final best model and explain what you can interpret from the model.

The code below performs model selection with `bestglm()` based on AIC. The 
categorical variables are coded such that one value is the reference category 
(here: Fair for `cut`, D for `color`, I1 for `clarity`)
and the rest are understood relative to this reference category. I.e. if `cut`
is Fair, we "do nothing", but if `cut` is Good, we add the coefficient estimate
for cutGood to our estimate of `logprice`. The best model is

$$
\begin{aligned}
\texttt{logprice} &\approx 2.99 + 1.58 \texttt{logcarat} + 0.0286
I(\texttt{cut} = Good) + 0.0405 I(\texttt{cut} = VeryGood) \\
& \quad + 0.0430 I(\texttt{cut} = Premium) + 0.0548 I(\texttt{cut} = Ideal) 
- 0.0218 I(\texttt{color} = E) \\
& \quad - 0.0389 I(\texttt{color} = F) - 0.0680 I(\texttt{color} = G) 
- 0.113 I(\texttt{color} = H) - 0.165 I(\texttt{color} = I) \\
& \quad - 0.224 I(\texttt{color} = J) + 0.175 I(\texttt{clarity} = SI2) 
+ 0.252 I(\texttt{clarity} = SI1) \\
& \quad + 0.316 I(\texttt{clarity} = VS2) + 0.346 I(\texttt{clarity} = VS1) 
+ 0.403 I(\texttt{clarity} = VVS2) \\
& \quad + 0.433 I(\texttt{clarity} = VVS1) + 0.474 I(\texttt{clarity} = IF) + 0.0693 \texttt{xx}
\end{aligned}
$$

The predictors included in the final model are `logcaract`, `cut`, `color`, 
`clarity` and `xx`. They are all significant (with any reasonable level), and so
is the model. The model explains 98.15\% of the variance in the data. The
coeffients all have the sign and order as expected, e.g. better clarity gives
a higher estimate of `logprice`.

**Q7:** Calculate and report the MSE of the test set using the best model (on the scale of `logprice`).  

We also calculate and print the MSE of the test set using the best model in the
following code, on the scale of `logprice`.

```{r,eval=TRUE}
ds=as.data.frame(within(dtrain,{
  y=logprice    # setting reponse
  logprice=NULL # not include as covariate
  price=NULL    # not include as covariate
  carat=NULL    # not include as covariate
}))

fit=bestglm(Xy=ds, IC="AIC")$BestModel
summary(fit)

ds_test=as.data.frame(within(dtest,{
  y=logprice    # setting reponse
  logprice=NULL # not include as covariate
  price=NULL    # not include as covariate
  carat=NULL    # not include as covariate
}))

preds = predict(fit, dplyr::select(ds_test, -y))
mse = mean((preds - ds_test$y)^2)
print(paste("MSE:", mse))
```

**Q8:** Build a model matrix for the covariates `~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1`. What is the dimension of this matrix?  

In the following code we build a model matrix for the covariates
`~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1`. The dimension of the
matrix is $5000 \times 24$; one column for each of `logcarat`, `depth`, `table`, `xx`,
`yy` and `zz`, five columns for `cut`, six for `color` 
and seven for `clarity`. The reason that we have five columns for `cut` and not
four, is that we have not included an intercept. Usually the reference level
for the categorical variables are captured by the intercept; now they will be
captured by the `cut` covariate instead.


```{r}
x_train = model.matrix(y ~ . -1, ds)
colnames(x_train)
x_test = model.matrix(y ~ . -1, ds_test)
```


**Q9:** Fit a lasso regression to the diamond data with `logprice` as the response and the model matrix given in Q8. How did you find the value to be used for the regularization parameter?  

In the following code we fit a lasso regression to the diamond data with
`logprice`as the response and the model matrix given in Q8. We use 
cross-validation (with `cv.glmnet`) to find the best value for the 
regularization parameter $\lambda$. We could use the one minizing average 
validation MSE over the folds. Another value we can choose the one resulting in
the simplest model that has error within one standard error of the estimated error
from the best model. We choose the latter, since the using the former might lead
to overfitting, and since in general simpler models are preffered when all else
is almost equal. The value is printed below. 

```{r}
set.seed(1)
cv = cv.glmnet(x_train, ds$y, alpha=1)  # alpha=1 for lasso
plot(cv)
print(paste("Lambda value:", cv$lambda.1se))
coef(cv)

```


**Q10:** Calculate and report the MSE of the test set (on the scale of `logprice`).  

The MSE of the model predictions on the test set is calculated and printed
below. It is somewhat higher than the error we obtained using `bestglm()` 
before.

```{r}
preds_lasso = predict(cv, x_test)
mse_lasso = mean((preds_lasso - ds_test$y)^2)
print(paste("MSE:", mse_lasso))
```


**Q11:** A regression tree to model is built using a _greedy_ approach. What does that mean? Explain the strategy used for constructing a regression tree.  

Building a regression tree in a _greedy_ manner means choosing the splitting
criterion that splits the data most successfully (by some criterion) at each step.
This might not result in the best possible tree overall. The complete algorithm:
Split recursively (like described) until some stop criterion is fullfilled (e.g.
number of samples per leaf node). If we want a smaller tree (avoid overfitting,
better interpretability), we can do some pruning. Then, for prediction, follow 
the tree all the way from root to the leaf nodes, and then use
the training exemplars at that leaf node to predict the value. Typically by
taking the mean.

**Q12:** Is a regression tree a suitable method to handle both numerical and categorical covariates? Elaborate.

Regression trees can handle both numerical and categorical covariates. At
splits we can consider splitting by category(/ies) or by some threshold for
numerical variables - a criterion can be e.g. $x_i < 50$. To not get an infinite
number of possible splits to consider, we can e.g. test the numerical variables
over a grid.

**Q13:** Fit a (full) regression tree to the diamond data with `logprice` as the response (and the same covariates as for c and d), and plot the result. Comment briefly on you findings.  

We fit a full regression tree to the diamond data with `logprice` as the
response and the same covariates as before, i.e. those in `ds`, and plot the
result in the code below.

```{r}
reg_tree = tree(y ~ ., ds)
plot(reg_tree, type="proportional")
text(reg_tree)
```

The dataset is first split by $\texttt{yy} < 5.665$. So if $\texttt{yy} < 5.665$ 
we go left, else right. Most of the reduction in impurity happens at the first step. 
More different values of logprice amongst exemplars means higher impurity,
measured by RSS in this case.
The tree is compact and easy to interpret. The tree only uses two of the
variables, `yy` and `xx`, which is interesting.

**Q14:** Calculate and report the MSE of the test set (on the scale of `logprice`).  

The MSE of the predictions by the regression tree fited in Q13 is calculated
below. The score is not very impressive compared to the previous models, which
is not surprising considering the tree (`yy` obviously tells us a lot about the
price, but we would have known by now if using almost only `yy` to predict the
price was a good way to go).

```{r}
preds_reg_tree = predict(reg_tree, dplyr::select(ds_test, -y))
mse_reg_tree = mean((preds_reg_tree - ds_test$y)^2)
print(paste("MSE:", mse_reg_tree))
```

**Q15:** Explain the motivation behind bagging, and how bagging differs from random forest? What is the role of bootstrapping?

Regression trees can be very sensitive to small changes in the data. E.g. if the
first split is different when the data is changed a little, we migth well not 
get any of the same rules for splitting into different regions. In other words:
Regression tree models have a high variance. 

To reduce the variance we would like to fit trees from different samples and
combine them (e.g. by taking the mean of predictions). Unfortunately we only
have one sample. What we can do is to estimate the true distribution by the
empirical distribution, and take boostrap samples to get several trees.

These "bootstrap trees" might still be quite similar, especially if there is one 
very strong predictor, so that the variance won't be reduced very much. Thus an idea 
is to just consider a random subset of the  covariates at each split, to get
more different trees. This is exactly what is done with random forests. 
The size of the subsets should be dependent on how dependent the covariates are 
on each other. For our data the covariates are probably
highly correlated, especially `carat`, `depth`, `table`,
`xx`, `yy` and `yy` (they are definitely dependent on each
other).

**Q16:** What are the parameter(s) to be set in random forest, and what are the rules for setting these?  

Other than the parameter $m$ for the size of the random subset to be considered
at each split (typically $p/3$ for regression), we need to choose the number of
trees $B$, which should be large enough for the forest to stabilize. There are
other parameters one can consider, like `nodesize` and `maxnodes`,
but we let these have the default values.
  
**Q17:** Boosting is a popular method. What is the main difference between random forest and boosting?  

Boosting works a little differently from bagging and random forests. First a
regression (we only discuss regression here) tree is fit with $d$ splits, $d$
typically not beeing very large (can be 1). Then we fit a new tree, say 
$\hat{f}^b(x)$, and a shrunken version $\lambda \hat{f}^b(x)$ (
$0 < \lambda < 1$, typically 0.1 or 0.01) is added to the previous model. Then
we repeat this procedure until satisfied. Now we need to take care when deciding
how many trees $B$ to fit, because with this method we can easily overfit the
training data.

**Q18:** Fit a random forest to the diamond data with `logprice` as the response (and the same covariates as before). Comment on your choice of parameter (as decribed in Q16). 

In the following code we fit a random forest to the diamond data with 
`logprice` as the response. There are 9 covariates, so we choose $m = 9/3 = 3$, 
and we make a guess that 500 trees should be sufficient.

```{r}
set.seed(1)
rf = randomForest(y ~ ., ds, mtry=3, ntree=500, importance=T)
```


**Q19:** Make a variable importance plot and comment on the plot. Calculate and report the MSE of the test set (on the scale of `logprice`).  

Below we make a variable importance plot from the random forest. The left
plot shows how much, averaged over all the trees, the prediction error on the
out-of-bag sample (exemplars not in the bootstrap sample) increases when the
values for the covariates are permuted. The right plot represents how much
impurity decreases in total
(averaged over all trees) for each of the variables. 

Somewhat surprisingly, the plots seem to disagree. One possible explanation is
that for the first node(s) `logcarat`, `xx`. `yy` and `zz` work very well for 
splitting the training data, so they are often chosen (when they are in the 
random subset considered at the split). But in the end, especially `clarity` 
and `color` are better predictors. 

Most likely a combination is needed to really get good predictions; a big
diamond which is not pretty is not worth a fortune, neither is a perfectly
looking tiny one. This claim is backed up by the fact that our random forest,
which is our only model capturing interactions between the more different 
covariates, is the best model at predicting the test
set. The decision tree only captures  interactions between `xx` and `yy`, and the 
previous models don't consider interactions. See the random forest MSE printed 
below. 

```{r}
varImpPlot(rf)
preds_rf = predict(rf, dplyr::select(ds_test, -y))
mse_rf = mean((preds_rf - ds_test$y)^2)
print(paste("MSE:", mse_rf))
```


**Q20:** Finally, compare the results from c (subset selection), d (lasso), e (tree) and f (random forest): Which method has given the best performance on the test set and which method has given you the best insight into the relationship between the price and the covariates?

As discussed in Q19, the random forest performs best when it comes to 
predicting the test set. This is most likely do to its ability to capture
interactions that none of the other models do (see Q19), and that it can do so
without overfitting the training data.

We saw the importance of especially `clarity` already with the subset selection
and lasso-models, together with the "size-dependent" covariates. The tree
didn't really tell us much, other than that splitting the diamonds into
different sizes (`yy`) is an easy way to make predictions that are not very 
good. The random forest shows the importance of interactions, which makes sense
intuitively (see Q19).
  
# Problem 2: Unsupervised learning [3 points]
  
**Q21:** What is the definition of a principal component score, and how is the score related to the eigenvectors of the matrix ${\hat {\bf R}}$. 
  
The idea of PCA (principal component analysis) is to use linear combinations of
the covariate values to reduce the dimension while keeping as much information as
possible. Specifically, information here means variability in the data. The first 
PC is the linear combination with maximal variance ($Z_{i1} = \phi_1 \cdot X_i$,
$i = 1, \dots, n$,
and we require $||\phi_1||_2 < 1$). The second principal is the one orthogonal to
the first maximizing variance (with $||\phi_2||_2 < 1$), and so on.

It turns out $\phi_1$ is the eigenvector of the the correlation matrix of 
$\matr{X}$ (or, equivalently, the covariance matrix of the standardized 
covariate matrix) corresponding to the largest eigenvalue. The j-th principal
component score of the i-th datapoint is $Z_{ij}$.

**Q22:** Explain what is given in the plot with title "First eigenvector". Why are there only $n=64$ eigenvectors and $n=64$ principal component scores?

```{r, fig.cap = "\\label{fig:firsteigvec} First eigenvector."}
library(ElemStatLearn)
X=t(nci) #n times p matrix
table(rownames(X))
ngroups=length(table(rownames(X)))
cols=rainbow(ngroups)
cols[c(4,5,7,8,14)] = "black"
pch.vec = rep(4,14)
pch.vec[c(4,5,7,8,14)] = 15:19

colsvsnames=cbind(cols,sort(unique(rownames(X))))
colsamples=cols[match(rownames(X),colsvsnames[,2])] 
pchvsnames=cbind(pch.vec,sort(unique(rownames(X))))
pchsamples=pch.vec[match(rownames(X),pchvsnames[,2])]

Z=scale(X)

pca=prcomp(Z)

plot(1:dim(X)[2],pca$rotation[,1],type="l",xlab="genes",ylab="weight",main="First eigenvector")
```

The plot with title "First eigenvector" shows the eigenvector corresponding to
the largest eigenvalue of the empirical correlation matrix $\hat{R}$ of the data
$X$. This is the same as the weights to get the first PC. Since there are $n=64$
observations, the rank of the correlation matrix is (at most) $n-1=63$. So there
are 63 non-zero eigenvalues with 63 corresponding eigenvectors and principal
components. That prcomp returns 64 PC scores is just an implementational
detail. The standard deviation of the 64th PC is `r pca$sdev[64]`; it explains
none of the variability in the data. In fact, all the variability is explained
by the 63 (first) PCs.

  **Q23:** How many principal components are needed to explain 80% of the total variance in ${\bf Z}$? Why is `sum(pca$sdev^2)=p`?  
  
  The fraction of the variance is found by the equation
  
$$
R^2 =\frac{\sum_{i=1}^M \lambda_i}{\sum_{j=1}^p \lambda_j}  
$$

```{r}
# calculating the number of PC to explain 80% of the variance
quotient_variance <- cumsum(pca$sdev^2)/sum(pca$sdev^2)
for (i in seq(1,length(quotient_variance))){
  if (quotient_variance[i]>=0.8){
    res_i = i
    break
  }
}
```

In he code above we find the number of principal components needed to explain 80% of the total variance. The result is `r res_i``. 
The reason `sum(pca$sdev^2)=p` is because of our scaling. All principal components explain all the variance (so the total variance is $p$) and the variances have been scaled to 1. 

  **Q24**: Study the PC1 vs PC2 plot, and comment on the groupings observed. What can you say about the placement of the `K262`, `MCF7` and `UNKNOWN` samples? Produce the same plot for two other pairs of PCs and comment on your observations.

In Figure \ref{fig:pc1pc2} we see the first \textit{principal component} on the x-axis versus the second \textit{principal component} on the y-axis. The `K562` cells, that we know are part of the `LEUKEMIA` familiy, are clustered with exactly these cells for both versions \textit{A} and \textit{B}. `MCF7` cells are part of the `BREAST` cancer familiy. In the figure it is clusterted together with one of these cells, but also together with `COLON` cells. The `BREAST` cancer cells don't really have any clustered effects in this plot, but the `COLON` cells are very clustered in the top right corner. In the top left corner there is a big cluster of many different cancer cells, `NSCLC`, `OVARIAN` and `RENAL` in particular. And within this cluster the `UNKnOWN` cells are placed. Which means that we can't really give a clear conclusion of which cancer familiy the `UNKNOWN` cells are part of. 

We plott the `PC32` vs. `PC33` in Figure \ref{fig:pc32pc33} for when the \textit{principal components} explain $80\%$ of the variance. We also plott the `PC63` vs. `PC64` for when the variance is as small as possible and when the null vector to transform the principal component is used. 
In these plots we can't see any significant clustering, that would classifiy the cancer cells in any particular way. 

```{r, fig.cap = "\\label{fig:pc1pc2} PC1 vs. PC2"}
plot(pca$x[,1],pca$x[,2],xlab="PC1",ylab="PC2",pch=pchsamples,col=colsamples)
legend("bottomright",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)
```

```{r, fig.cap = "\\label{fig:pc32pc33} PC32 vs. PC33"}
plot(pca$x[,32],pca$x[,33],xlab="PC32",ylab="PC33",pch=pchsamples,col=colsamples)
legend("bottomright",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)
```

```{r, fig.cap = "\\label{fig:pc63pc64} PC63 vs. PC65"}
plot(pca$x[,63],pca$x[,64],xlab="PC63",ylab="PC64",pch=pchsamples,col=colsamples)
legend("bottomright",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)
```


**Q25:**: Explain what it means to use Euclidean distance and average linkage for hierarchical clustering. 

We first look at the average linkage. The algorithm for hierarchical clustering is given by

\begin{enumerate}
\item \textbf{Given:}
\item A set of $n$ observations $\matr{X}$
\item A dissimiliarity function \texttt{f(x)}
\item \texttt{While $\#\text{clusters} > 1$; Do:}
\begin{enumerate}
\item Find two clusters that are the least dissimilar
\item Create new cluster of these
\item Remove remove previous cluster
\item \texttt{End do;}
\end{enumerate}
\end{enumerate}

In this algoritm we see that we need some kind of dissimiliarity measure, and in this exercise we use the average linkage. The average linkage is in plain words the \textit{mean inter-cluster dissimiliarity}, given by the equation
$$
L(l,r) = \frac{1}{n_l n_r}\sum\limits_{i = 1}^{n_l}\sum\limits_{j=1}^{n_r}||\vect{x}_{li}-\vect{x}_{rj}||_2 .
$$
Where $l$ is the set of points in one cluster and $r$ is the set of points in the second cluster. The dissimilarity calculation inside the $||\cdot||_2$ is in our case the Euclidean distance. This is to capture the physical distance between two points in the space. Within a cluster one wants to minimize the variance between the observations to find the best possible cluster for each observation. This is done by the following equation, also using the Euclidean distance as a dissimilarity measure, 
$$
 L(C_k) = \frac{1}{n_{C_k}} \sum\limits_{i,j\in C_k} \sum\limits_{p=1}^P ||x_{ip}-x_{jp}||_2^2.
$$
with $n_{C_k}$ being the number of points in cluster number $k$, with the points having dimension $P$. The Euclidean distance is given by $||x_1- x_2||_2 = \sqrt{(x_1^2 + x_2^2)}$. Using this on the scaled observations $\matr{Z}$, one would end up with the partition shown in Figure \ref{fig:dendogram}.

**Q26:**: Perform hierarchical clustering with Euclidean distance and average linkage on the scaled gene expression in `Z`. Observe where our samples labelled as `K562`, `MCF7` and `UNKNOWN` are placed in the dendrogram. Which conclusions can you draw from this?
```{r, fig.cap = "\\label{fig:dendogram} Dendogram of hierarchical clustering with euclidean distance and average linkage on $\\matr{Z}$"}
library(ggplot2)
library(ggdendro)
library(tidyverse)
library(dendextend)
hc.average=hclust(dist(Z), method="ave")
str(hc.average)
dhc <- as.dendrogram(hc.average,hang=FALSE,dLeaf = NULL)
ddata <- dendro_data(dhc, type = "rectangle")
tmpx = round(segment(ddata)$xend)
tmpy = segment(ddata)$yend
res = numeric(length = length(label(ddata)$x))+300
for (i in seq(1,length(tmpx))){
  if (res[tmpx[i]]>tmpy[i]){
     res[tmpx[i]] = tmpy[i]
  }
}
res
col.text <- rep("red",64)
col.text[(label(ddata)$label == "UNKNOWN")|
           (label(ddata)$label == "K562A-repro")|
           (label(ddata)$label == "K562B-repro")|
           (label(ddata)$label == "MCF7A-repro")|
           (label(ddata)$label == "MCF7D-repro")] = "blue"

p <- ggplot(segment(ddata))+
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend))+
  geom_text(data = label(ddata), angle = 90,
            aes(x = x, y = res, label = label),colour = col.text, vjust = 0.5,hjust = 1, size = 2)+
  scale_colour_discrete(c("blue","red"))+
  ylim(c(0,max(segment(ddata)$y)))+
  theme_minimal()+
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks = element_blank(),
        axis.line.y = element_line(color = "black", 
                                   size = 1, linetype = "solid"))
  
p
```
The `K562` cells are clearly clustered together with only the `LEUKEMIA` cells, which is what we should expect because `K562` is in the `LEUKEMIA`family. Looking at the `MCF7` they are clustered together with `BREAST` cells, though these cells are spread out on the right side of the tree, it fits the fact that we know they `MCF7` are part of the `BREAST` familiy. The `UKNOWN`cells are clustered together with the `OVARIAN` cells, it also branches in to a bigger cluster of `OVARIAN` leafs. It is also in close connection to `NSCLC` and `MELANOMA`, but looks to either be a `OVARIAN` cell or in familiy with. 
  
  **Q27:**: Study the R-code and plot below. Here we have performed hierarchical clustering based on the first 64 principal component instead of the gene expression data in `Z`. What is the difference between using all the gene expression data and using the first 64 principal components in the clustering? We have plotted the dendrogram together with a heatmap of the data. Explain what is shown in the heatmap. What is given on the horizontal axis, vertical axis, value in the pixel grid?

From earlier questions we know that the principal components can be found by the equation $\vect{y}_k = \vect{z}_k \matr{V}$. The most significant difference between using the principal components versus the scaled observations, is computation time. The distanes betweeen points in the PC-space is the same as in the orthogonal space, because of the orthonormal matrix. 

In the figure below the result of running the hierarchical clustering algorithm with the principal components is shown. On the left side of the figure a dendogram explaining the partitioning is displayed. The rows in both the dendogram and the heatmap corresponnds to the names of the cells on the vertical axes to the right in the figure. Along the horistonal axes the principal components are displayed, with `PC1` furthest to the left. The principal components and the cell types form a grid system in the main plot, and the color of which corresponds to the \textit{principal component scores}. Red color indicates high positive scores and blue colors indicate high negative scores while zero score is a light yellow or white color. We can by this knowledge say that there are more variances in the first few principal components and they become more similiar in the later principal components, with the last elements $64$ having $0$ variance, which corresponds to the points having the same color. This what we concluded earlier in the exercise, only explained visually. 
```{r}
library(pheatmap)
npcs=64 
pheatmap(pca$x[,1:npcs],scale="none",cluster_col=FALSE,cluster_row=TRUE,clustering_distance_rows = "euclidean",clustering_method="average",fontsize_row=5,fontsize_col=5)
```

# Problem 3: Flying solo with diabetes data [6 points]

```{r}
flying=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/flying.dd")
ctrain=flying$ctrain
ctest=flying$ctest
```

**Q28:** Start by getting to know the _training data_, by producing summaries and plots. Write a few sentences about what you observe and include your top 3 informative plots and/or outputs.

# Exploratory data analysis

We start off by looking at the correlation matrix, seeing as this can provide insight into the correlation between the different covariates.  High correlation might indicate that two covariates encode similar information and that it might be beneficial to drop one of the covariates in order to reduce the complexity and variance. From the figure below it's clear that $\textrm{skin}$ and $\textrm{bmi}$ is very highly correlated, as well as $\textrm{age}$ and $\textrm{npreg}$.

```{r, fig.cap="Correlation matrix"}
library(dplyr)
library(tibble)
library(magrittr)
library(forcats)
library(purrr)
library(ggplot2)
library(tidyr)

train_df <- ctrain %>%
  as_tibble() %>%
  mutate(diabetes = as_factor(diabetes))

train_df %>%
  GGally::ggcorr()
```
We proceed by splitting the dataset into two parts, one where $\textrm{diabetes} = 1$ and another one where $\textrm{diabetes} = 0$. By creating boxplots for each of these cases we can inspect if there's any observable difference between the mean and variance of the covariates when the data is grouped by the value of $\textrm{diabetes}$. We immediately observe that the mean $\textrm{glu}$ is considerably higher for the group where $\textrm{diabetes} = 1$. The differences are not as clear for the other covariates, but it's worth noting that the mean value of $\textrm{skin}$ and $\textrm{bp}$ also seems to higher for the group with $\textrm{diabetes} = 1$.

```{r, fig.cap="Boxplot for dataset grouped by $\\textrm{diabetes}$"}

train_df %>%
  gather("variable", "value", c(-diabetes)) %>%
  ggplot(aes(y=value, x=variable)) +
  geom_boxplot() + 
  facet_wrap(~diabetes)
```

The next plot provides further insight into the importance of $\textrm{glu}$ and $\textrm{skin}$. There's a clear structure in the plot below, showing that the fraction of people with diabetes increases as the values of $\textrm{skin}$ and $\textrm{glu}$ increase.
There's 

```{r}
train_df %>%
  ggplot(aes(x = glu, y = skin, color = diabetes)) +
  geom_jitter()
```



**Q29:** Use different methods to analyse the data. In particular use 

* one method from Module 4: Classification
* one method from Module 8: Trees (and forests)
* one method from Module 9: Support vector machines and, finally
* one method from Module 11: Neural networks

For each method you 

* clearly write out the model and model assumptions for the method
* explain how any tuning parameters are chosen or model selection is performed
* report (any) insight into the interpretation of the fitted model
* evaluate the model using the test data, and report misclassifiation rate (cut-off 0.5 on probability) and plot ROC-curves and give the AUC (for method where class probabilities are given).

We start off by defining a number of functions that simplify tuning through the use of cross - validation. 

```{r}
#Input: A parsnip model, from the package parsnip, and a data frame where
#each row corresponds to a CV - iteration, i.e. a specific validation set.
#The folds dataframe is a modified version of the dataframe returned by
#vfold_cv from the rsample - package.

#Output: The folds dataframe, but this with columns containing 1) models fitted on the training data
#for the corresponding iteration and 2) predictions on the corresponding validation set

predict_on_folds <- function(model, folds) {
  prediction_df <- folds %>%
    mutate(
      fitted_model =
        map2(
          recipe,
          train,
          ~ model %>%
            fit(
              formula(.x),
              data = bake(
                object = .x,
                new_data = .y
              ),
            )
        ),
      prediction =
        pmap(
          lst(
            recipe,
            test,
            fitted_model
          ),
          function(recipe, test, fitted_model)
            fitted_model %>%
              predict(
                new_data = bake(
                  object = recipe,
                  new_data = test
                ),
                type = "class"
              )
        ),
      test_estimate = prediction %>%
        map(".pred_class")
    )
  return(prediction_df)
}


# Input: Data frame containing truths and estimates in each row
# Output: Average accuracy across all rows
validation_score <- function(cv_df) {
  cv_df %>%
    select(test_truth, test_estimate) %>%
    pmap(
      function(
                     test_truth,
                     test_estimate)
        tibble(
          test_truth,
          test_estimate
        )
    ) %>%
    map(accuracy,
      truth = test_truth,
      estimate = test_estimate
    ) %>%
    map_dbl(".estimate") %>%
    mean()
}
```

We do some additional preparation that simplify preprocessing and ensures that the input is consistent across all methods. We define two different "recipes", one that centers and scales the data have mean zero and unit variance and one that does not. 

```{r}
library(recipes)
library(rsample)

test_df <- ctest %>%
  as_tibble() %>%
  mutate(diabetes = as_factor(diabetes))

unscaled_rec <- train_df %>%
  recipe() %>%
  update_role(diabetes, new_role = "outcome") %>%
  update_role(-diabetes, new_role = "predictor")

unscaled_prepped <- unscaled_rec %>%
  prep(retain = TRUE)

train_unscaled <- unscaled_prepped %>%
  juice()

test_unscaled <- unscaled_prepped %>%
  bake(test_df)

unscaled_folds <- train_df %>%
  vfold_cv(v = 5) %>%
  mutate(
    recipe = splits %>%
      map(prepper, recipe = unscaled_rec),
    train = splits %>%
      map(analysis),
    test = splits %>%
      map(assessment),
    test_truth = test %>%
      map("diabetes")
  )

scaled_rec <- train_df %>%
  recipe() %>%
  update_role(diabetes, new_role = "outcome") %>%
  update_role(-diabetes, new_role = "predictor") %>%
  step_center(-diabetes) %>%
  step_scale(-diabetes)


scaled_prepped <- scaled_rec %>%
  prep(retain = TRUE)

train_scaled <- scaled_prepped %>%
  juice()

test_df <- ctest %>%
  as_tibble() %>%
  mutate(diabetes = as_factor(diabetes))

test_scaled <- scaled_prepped %>%
  bake(test_df)

scaled_folds <- train_df %>%
  vfold_cv(v = 5) %>%
  mutate(
    recipe = splits %>%
      map(prepper, recipe = scaled_rec),
    train = splits %>%
      map(analysis),
    test = splits %>%
      map(assessment),
    test_truth = test %>%
      map("diabetes")
  )
```



## Logistic regression

The first method we want to explore is logistic regression. Logstic regression assumes that the response takes on values in $\{0, 1\}$. In addition, we assume that $Y_i, i = 1,\ldots, n$ is Bernoulli - distributed with sucess probability $p_i$. The logistic function has the form

$$
p_i = \frac{\exp{\left(\beta_0 + \sum_{n=1}^q\beta_nx_{n}\right)}}{1 + \exp{\left(\beta_0 + \sum_{n=1}^q\beta_nx_n\right)}},
$$
where $q$ is the number of covariates. In our case we have $q = 7$.  The different values of $\mathbf{\beta}$ are obtained by maximizing the likelihood function $L(\mathbf{\beta} \mid \ \mathbf{x}) = \prod_{i=1}^nf(x_i \mid \mathbf{\beta})$, where $f(x_i \mid \mathbf{\beta})$ is the Bernoulli - pmf with parameter $p_i$ as shown above. The maximation is done using a numerical optimization algorithm - typically by the use of either Fisher scoring or the Newton-Rahpson method. 

It's not necessarily desirable to include all covariates in the model. In order to find a model that performs well without being unnecessarily complex we calculate the AIC for the possible models and choose the model that maximizes AIC as the final model. 


```{r}
library(bestglm)
library(yardstick)

best_glm <- bestglm::bestglm(
  ctrain %>%
    select(-diabetes, diabetes),
  family = binomial,
  IC = "AIC"
)$BestModel

summary(best_glm)

broom::tidy(best_glm) %>% select(term, estimate, p.value) %>% knitr::kable()
```

From the estimated values $\mathbf{\beta}$ shown above we can note that increased values for any of the covariates lead to higher values of $p_i$. It can also be seen that not all of the included covariates are significant. It's clear from the estimates and the related standard errors that $\textrm{bmi}$ and $\textrm{glu}$, which was identified as important during the exploratory phase, are important. It's worth noting that $\textrm{skin}$ is excluded, but this might make sense seeing as it was highly correlated with $\textrm{bmi}$. 

In the following snippet of code we test our model on the test set and report the accuracy and the AUC. We also include the confidence matrix and the ROC curve.

```{r}
class_pred <- tibble(
  estimate = best_glm %>%
    predict(
      ctest %>%
        select(
          -diabetes, diabetes
        ),
      type = "response"
    ) %>%
    map_dbl(~ .x > 0.5) %>%
    as.factor(),
  truth = test_df %>%
    pull(diabetes)
)

class_prob_pred <- tibble(
  estimate = best_glm %>%
    predict(
      ctest %>%
        select(
          -diabetes, diabetes
        ),
      type = "response"
    ),
  truth = test_df %>%
    pull(diabetes)
)

accuracy(class_pred, estimate, truth) %>%
  select(.metric, .estimate) %>%
  knitr::kable(col.names = c("Metric", "Estimate"))

roc_curve(class_prob_pred, truth, estimate) %>%
  autoplot()

roc_auc(class_prob_pred, truth, estimate) %>%
  select(.metric, .estimate) %>%
  knitr::kable(col.names = c("Metric", "Estimate"))

conf_mat(class_pred, truth, estimate) %>%
  autoplot("heatmap")
```

## Random forest

We proceed by fitting a random forest to our data. A random forest consists of $B$ decision trees. The decision tree has parameters determing how many samples there needs to be in a leaf node before we can split further. This is a tuning parameter, called $\texttt{min\_n}$ in the code below. Only $m, m < p$ randomly chosen covariates can be considered at each split. This is also a tuning parameter, called $\texttt{mtry}$ below. When splitting a node we want to minimize the Gini index, given by

$$
G = \sum_{k=1}^K\hat{p}_{jk}(1-\hat{p}_{jk})
$$
for leaf node $j$. 

The parameters are found through the use of K - fold cross validation, with $K = 5$. The parameter search is by no means exhaustive, but the should be sufficient for our purpose. The number of trees is fixed at $B = 100$, seeing as this is not a tuning parameter and it's common to simple set to be "high enough". By taking a look at the mean decrease in the Gini index for the diffiferent covariates, we observe that $\textrm{bmi}$ and $\textrm{glu}$ are, again, very important. The covariates $\text{ped}$ and $\text{age}$ also seem to be fairly important.

```{r}
library(parsnip)

n_predictors <- unscaled_rec$var_info %>%
  filter(role == "predictor") %>%
  nrow()

# Defining grid used for hyperparameter search.
rf_param_df <- cross_df(list(
  mtry = seq(1, n_predictors),
  trees = list(100),
  min_n = seq(1, 20, 1)
))

# Creating a list of models corresponding to the different choices
# of hyperparameter values
model_list <- rf_param_df %>%
  as.list() %>%
  pmap(
    function(
                 mtry,
                 trees,
                 min_n)
      rand_forest(
        mode = "classification",
        mtry = mtry,
        trees = trees,
        min_n = min_n
      ) %>%
        set_engine("randomForest")
  )

# Fitting the different models
cv_list <- model_list %>%
  map(predict_on_folds,
    folds = unscaled_folds
  )


# Evaluating the performance of the different models by extracting the
# mean accuracy across all folds
rf_param_df <- rf_param_df %>% mutate(accuracy = cv_list %>%
  map_dbl(validation_score))

best_accuracy <- rf_param_df %>%
  filter(accuracy == max(accuracy)) %>%
  select(accuracy)

best_params <- rf_param_df %>%
  mutate(index = row_number()) %>%
  filter(accuracy == max(accuracy)) %>%
  select(mtry, trees, min_n) %>%
  head(1)

best_params %>%
  select(mtry, min_n) %>%
  knitr::kable(caption="Best parameters for random forest")


# Fitting a model using the entire test set, with
# parameters as found through cross-validation
rf_estimator <- rand_forest(
  mode = "classification",
  mtry = best_params %>% pull(mtry),
  trees = best_params %>% pull(trees),
  min_n = best_params %>% pull(min_n)
) %>%
  set_engine("randomForest") %>%
  fit(
    formula = formula(unscaled_prepped),
    data = train_unscaled
  )

rf_estimator$fit$importance %>% knitr::kable()
```

The accuracy and AUC is shown below, together with plots of the ROC curve and the confusion matrix.


```{r}

results <- tibble(
  estimated_class = rf_estimator %>%
    predict(
      new_data = test_unscaled,
      type = "class"
    ) %>%
    pull(.pred_class),
  estimated_class_probs = rf_estimator %>%
    predict(
      new_data = test_unscaled,
      type = "prob"
    ) %>%
    pull(.pred_1),
  truth = test_unscaled %>%
    pull(diabetes)
)

accuracy(results, truth, estimated_class) %>%
  select(.metric, .estimate) %>%
  knitr::kable(col.names = c("Metric", "Estimate"))


roc_auc(results, truth, estimated_class_probs) %>%
  select(.metric, .estimate) %>%
  knitr::kable(col.names = c("Metric", "Estimate"))

roc_curve(results, truth, estimated_class_probs) %>%
  autoplot()

conf_mat(results, truth, estimated_class) %>%
  autoplot("heatmap")
```
## Support vector machine

We proceed by considering a support vector classifier. This algorithms constructs a hyperplane that attempts to separate the data. The hyperplane is constructed by maximizing the margin $M$ subject to a number a constraints. 

The hyperplane is defined by $f(\mathbf{x} \mid \mathbf{\beta}) = \beta_0 + \sum_{i=1}^n\beta_ix_i = 0$. By calculating $\textrm{sign}\left(f(\mathbf{x} \mid \mathbf{\beta})\mid_{\mathbf{x}=\mathbf{x}_{i}}\right)$ we obtain the predicted class, given that classes are encoded as $Y \in \{-1, 1\}$. In other words, the observations are classified by looking at which side of the hyperplane the observation is located. 

Based on our observation in the first section, where we observed that $\text{skin}$ and $\text{skin}$ seems to increase the chance of $\text{diabetes} = 0$, we choose a polynomial decision boundary. The scatter plot of the mentioned covariates indicates that a linear decision boundary could do a good job of predicting the value of $\text{diabetes}$. 

The procedure for finding the hyperplane consists of maximizing the margin $M$ subject to a set of constraints. The margin $M$ determines how far away from the hyperplane a point must be before it stops having an impact on the construction of the hyperplane. The related optimization problem becomes 

\begin{align*}
\max_{\mathbf{\beta}, \mathbf{\epsilon}}M\quad\text{ subject to} \\
\mathbf{\beta}^T\mathbf{\beta} = 1 \\
y_i(\beta_0 + \sum_{i=1}^n\beta_ix_{ip}) \geq M(1 - \epsilon_i) \ \forall i = 1,\ldots,n\\
\epsilon_i \geq 0 \\
\sum_{i=1}^n\epsilon_i \leq C
\end{align*}

We use K - fold cross validation to determine the parameters, with $K = 5$. 

```{r}

svm_param_df <- cross_df(
  list(
    cost = map(seq(-3, 2, 1), ~ 10^(.)),
    degree = seq(1, 4)
  )
)

model_list <- svm_param_df %>%
  as.list() %>%
  pmap(
    function(
                 cost,
                 degree)
      svm_poly(
        mode = "classification",
        cost = cost,
        degree = degree
      ) %>%
        set_engine("kernlab")
  )

cv_list <- model_list %>% map(predict_on_folds,
  folds = scaled_folds
)

svm_param_df <- svm_param_df %>%
  mutate(accuracy = cv_list %>%
    map_dbl(validation_score))

svm_param_df %>%
  filter(accuracy == max(accuracy)) %>%
  select(cost, degree) %>% 
  knitr::kable(caption = "Best parameters for SVC")

best_accuracy <- svm_param_df %>% filter(accuracy == max(accuracy)) %>% select(accuracy)

best_params <- svm_param_df %>%
  mutate(index = row_number()) %>%
  filter(accuracy == max(accuracy)) %>%
  select(cost, degree) %>%
  head(1)

svm_estimator <- svm_poly(
  mode = "classification",
  cost = best_params %>% pull(cost),
  degree = best_params %>% pull(degree)
) %>%
  set_engine("kernlab") %>%
  fit(
    formula = formula(scaled_prepped),
    data = train_scaled
  )


results <- tibble(
  estimated_class = svm_estimator %>%
    predict(
      new_data = test_scaled,
      type = "class"
    ) %>%
    pull(.pred_class),
  estimated_class_probs = svm_estimator %>%
    predict(
      new_data = test_scaled,
      type = "prob"
    ) %>%
    pull(.pred_1),
  truth = test_scaled %>%
    pull(diabetes)
)

accuracy(results, truth, estimated_class) %>%
  select(.metric, .estimate) %>%
  knitr::kable(col.names = c("Metric", "Estimate"))


roc_auc(results, truth, estimated_class_probs) %>%
  select(.metric, .estimate) %>%
  knitr::kable(col.names = c("Metric", "Estimate"))

roc_curve(results, truth, estimated_class_probs) %>%
  autoplot()

conf_mat(results, truth, estimated_class) %>%
  autoplot("heatmap")
```

## Neural network

A fully-connected feedforward neural network consists of a series of hidden layers, with each layer consisting of a given number of units. Given an input vector $\mathbf{X}$, each unit in the first layer computes an output $\phi\left(\mathbf{X}\mathbf{w}_i + b_i\right)$. The function $\phi$ is referred to as the activation function. The output of the entire layer is then either passed to another hidden layer, where the process is repeated, or used as the final output. The weights of the different layers in the network, $\mathbf{W} = \begin{bmatrix} w_1,\ldots,w_n\end{bmatrix}^T$ and $\mathbf{b}$, can be estimated by minimizing a loss function with respect to the weights. This is done by computing the gradient of the loss function through a technique known as backpropagation, and applying a numerical optimization method in order to minimize the loss function. 

In the piece of code below we will fit a neural network with a single layer, which is also known as a multi-layer perceptron. The number of units and weight regularization parameter in this layer is chosen through the use of cross-validation with $K = 5$ folds. The weight regularization parameter changes the loss function by penalizing the size of the weights. We use the softmax - function as the activation function. The cross-validation with a single layer is done below. 


```{r}

nn_param_df <- cross_df(
  list(
    units =  c(1, 2, 4, 8),
    penalty = seq(-1, 3) %>% map(~10^-.x)
    
  )
)

model_list <- nn_param_df %>%
  as.list() %>%
  pmap(
    function(
                 units,
                 penalty)
      mlp(
        mode = "classification",
        hidden_units = units,
        penalty = penalty
      ) %>%
        set_engine("nnet")
  )

cv_list <- model_list %>% map(predict_on_folds,
  folds = scaled_folds
)

nn_param_df <- nn_param_df %>%
  mutate(accuracy = cv_list %>%
    map_dbl(validation_score))

nn_param_df %>%
  filter(accuracy == max(accuracy)) %>% 
  select(units, penalty) %>%
  knitr::kable(caption = "Best parameters for NN")

best_accuracy <- nn_param_df %>% filter(accuracy == max(accuracy)) %>% select(accuracy)

best_params <- nn_param_df %>%
  mutate(index = row_number()) %>%
  filter(accuracy == max(accuracy)) %>%
  select(units, penalty) %>%
  head(1)

nn_estimator <- mlp(
  mode = "classification",
  hidden_units = best_params %>% pull(units),
  penalty = best_params %>% pull(penalty)
) %>%
  set_engine("nnet") %>%
  fit(
    formula = formula(scaled_prepped),
    data = train_scaled
  )


results <- tibble(
  estimated_class = svm_estimator %>%
    predict(
      new_data = test_scaled,
      type = "class"
    ) %>%
    pull(.pred_class),
  estimated_class_probs = svm_estimator %>%
    predict(
      new_data = test_scaled,
      type = "prob"
    ) %>%
    pull(.pred_1),
  truth = test_scaled %>%
    pull(diabetes)
)

accuracy(results, truth, estimated_class) %>%
  select(.metric, .estimate) %>%
  knitr::kable(col.names = c("Metric", "Estimate"))


roc_auc(results, truth, estimated_class_probs) %>%
  select(.metric, .estimate) %>%
  knitr::kable(col.names = c("Metric", "Estimate"))

roc_curve(results, truth, estimated_class_probs) %>%
  autoplot()

conf_mat(results, truth, estimated_class) %>%
  autoplot("heatmap")
```

**Q30:** Conclude with choosing a winning method, and explain why you mean that this is the winning method.

Logistic regression achieves the highest accuracy and the highest AUC. The other methods are not far behind, but has additional disadvantages that make them less attractive. Logistic regression is fairly easy to interpret due to relation between the success probability and the values of the covariates. The random forest is harder to interpret due to the high number of trees that are involved. The random nature of the random forest is also a downside. The support vector classifier is very easy to interpret for $p = 2$ and $p = 3$, but becomes harder to both visualize and explain for high-dimensional data. Neural networks are notoriously hard to interpret due to it's very complex structure. This, in combination with it's inability to reliably perform better than the logistic regression, makes it less desirable than the logistic regression in this case. When taking all of this into account we're led to choose the logistic regression as the best model for this particular problem. 
